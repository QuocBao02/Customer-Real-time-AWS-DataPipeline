[
{
	"uri": "/2-kinesis/accesskey/",
	"title": "Create AWS Access Key",
	"tags": [],
	"description": "",
	"content": "Create AWS Access Key  In this project, we will interact with AWS using the CLI (Command Line Interface). Therefore, we need to create an Access Key to enable this connection.  Step-by-Step Guide:  Sign in to AWS Management Console and go to search box \u0026ldquo;IAM\u0026rdquo;  Select Manage access keys to create new key  Select Create access key  For this project, I will grant root permission for this key. Additionally, you can create an IAM user for specific purposes. Click Create Access key to proceed.  Once the Access Keys is created, be sure to note it down and keep it confidential. For more convenience, you can download the access key information as a CSV file.  Well done!  "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "This project aims to build a real-time data pipeline for customer data on an e-commerce platform using AWS services. This will allow the owner to gain clear insights into customer demographics(age, gender, locations, etc.), product consumption patterns, best-selling products, and more. With this information, the owner can make informed decisions to benefit the company.\nSystem Infrastructure  Data source: Data will be crawled from customers who made the transactions. In this case, i will use the Fake Python library to generate customer data. Data Ingestion: AWS Kinesis, after data is generated, it will be sent to AWS Kinesis using the boto3 Python library and an AWS Access Point Processing data : AWS Lambda will be trigged by AWS Kinesis when new data arrives. Lambda acts as a transformer, moving data from AWS Kinesis to AWS S3 Storange data( Data Lake): AWS S3 will save the raw data, acting as a Data Lake to maintain data purity. Database(Data Warehouse): AWS Redshift serves as the data warehouse, containing relational customer data. A Lambda function will be trigged to perform ELT tasks when new data arrives in S3. Analystic (Visualization): AWS QuickSight provides an overview of customer activities and product sales to the Owner. QuickSight will create dashboard to help the Owner make informed the decisions to benefit the company.  Data Source Overview The format of customer data looks like:\nData Description: Customer Data\rExample: Name: 'Laura Salazar'\rAddress: '98201 Moore Cliffs Apt. 537 Jamesburgh, NV 21021'\rPhone: '763-957-9359x022'\rAge: 20\rGender: M or F\rProduct Name: 'Canon EOS R5'\rAmount: 100\rPurchase Date: '2010-01-11'\r"
},
{
	"uri": "/2-kinesis/",
	"title": "Kinesis Data Ingestion",
	"tags": [],
	"description": "",
	"content": " In this section, I will create an AWS Access Key to connect my local data generator and send data to Kinesis using boto3 Python library  Overview of Boto3  Boto3 is the Amazon Web Services (AWS) SDK for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2. In this project, boto3 is used to interact with Amazon Kinesis, enabling the sending of real-time data to the Kinesis stream programmatically.  Amazon Kinesis Data Stream   Amazon Kinesis is a platform on AWS to collect, process, and analyze real-time, streaming data.\n  Kinesis allows me to ingest customer data in real time from various sources such as web applications, mobile apps, or IoT devices. In this project, I used local data generator for data source.\n  Contents  Create AWS Access Key Generate Customer Data Create Kinesis Stream Key  "
},
{
	"uri": "/3-lambda/",
	"title": "Lambda Data Processing",
	"tags": [],
	"description": "",
	"content": " In this section, I will define two Lambda functions. The first retrieves the latest data data from Kinesis and saves it to an S3 bucket for the data lake. The second function then extracts, transforms, and loads the newly added objects in the S3 bucket to Redshift, a relational database for the data warehouse.  Lambda  AWS lambda is a cloud service designed to execute code in response to specific events and automatically manages the underlying compute resources. We simply upload your code in the form of a Lambda function, and the service takes care of everything required to run and scale the code with high availability.   Contents  Get Data From Kinesis to S3 ETL Data From S3 to Redshift  "
},
{
	"uri": "/3-lambda/lambda-to-s3/",
	"title": "Load Data From Kinesis to S3",
	"tags": [],
	"description": "",
	"content": " In this section, I will walk you through the steps involved in setting up the data pipeline: Creating an S3 bucket for the data lake, an IAM role for the Lambda function to access S3 and other project services, a Lambda function to move data from Kinesis to S3, and a trigger for this Lambda function. Let\u0026rsquo;s dive into the details below.  Amazon Simple Storage Service - S3  Amazon S3 is an object storage service that provides high scalability, data availability, security, and performance. It\u0026rsquo;s one of the core services offered by AWS and is used by millions of customers to store and manage data for various use cases:  Step-by-Step Guide: Create Lambda Role  Go to IAM service management console  Select Create role to create new role for Lambda function.  Select Lambda for AWS service, then click Next.  Add CloudwatchFullAccess to Permission policies.  Add AmazonS3FullAccess to Permission policies.  Add AmazonKinesisFullAccess to Permission policies.  Add AmazonRedshiftFullAccess to Permission policies.  Set Role name is full-customer-data-stream-role and click Create role.     Create S3 bucket for the data lake  Go to S3 service management console  Select Create bucket to create new bucket. Set Bucket name is customer-data-datalake-quocbao, It must be unique within the global namespace and follow the bucket naming rules. Then, Click Create bucket.  Well done!, S3 bucket is created!     Create Lambda function to move data from Kinesis to S3  Go to Lambda service management console  Select Create a function to create new lambda function.  Set Function name is kinesis_event_processor_to_s3, Runtime type is Python 3.12, Architecture is x86_64. Select Use an existing role to set my lambda role was create before. Then click Create function.    Add lambda function code.  Select Code and paste the code source below before clicking Deploy.  import json import boto3 import time def generate_partition(): \u0026#39;\u0026#39;\u0026#39;Create directory to save data into data lake in hadoop hdfs\u0026#39;\u0026#39;\u0026#39; current_time=time.strftime(\u0026#39;%Y-%m-%d%H:%M:%S\u0026#39;, time.localtime()) date_string = current_time.split()[0].split(\u0026#39;-\u0026#39;) time_string = current_time.split()[1].split(\u0026#39;:\u0026#39;) year=date_string[0] month=date_string[1] day=date_string[2] hour=time_string[0] minute=time_string[1] second=time_string[2] prefix = \u0026#34;data-\u0026#34; + day + \u0026#34;-\u0026#34; + month + \u0026#34;-\u0026#34; + year + \u0026#34;/\u0026#34; data=\u0026#34;event-\u0026#34;+hour+\u0026#34;-\u0026#34;+minute+\u0026#34;-\u0026#34;+second+\u0026#34;.json\u0026#34; return prefix+data s3_cli = boto3.client(\u0026#34;s3\u0026#34;) bucket_name = \u0026#34;customer-data-datalake-quocbao\u0026#34; def lambda_handler(event, context): object_name = generate_partition() # print(event) # print(object_name) s3_cli.put_object( Bucket=bucket_name, Key=object_name, Body=json.dumps(event).encode(\u0026#34;utf-8\u0026#34;) ) return { \u0026#39;statusCode\u0026#39;: 200 }  Add trigger to Lambda function  Select Add trigger and choose Kinesis source.  Set Kinesis stream is kinesis/CustomersDataStream, the kinesis stream key was created before. Batch size is 1. Click Add to finish add trigger for kinesis_event_processor_to_s3     "
},
{
	"uri": "/4-redshiftvisualize/",
	"title": "Running Data Pipeline and Visualization Redshift Data",
	"tags": [],
	"description": "",
	"content": " In this section,we will run all the steps of our data pipeline to gain an overview of the project.Then, we will use Redshift chart to get basic insights into the Customer Data stored in our Redshift Data Warehouse  Run all the steps of Data Pipeline.  Run Data Generator  Data is stored in S3 data lake.  Data is stored in Redshift datawarehouse.   Redshift Chart  Distribution of Customers by Gender  select gender, count(*) as amount from customers group by gender; 2. Top 10 Products by Sales\nselect product_name, count(*) from products group by product_name order by count(*) desc limit 10; 3. Number of Sales Over Time\nselect purchase_date, amount from transactions order by purchase_date; 4. Age distribution of Customers\nselect age, count(*) from customers group by age order by age; "
},
{
	"uri": "/3-lambda/lambda-to-redshift/",
	"title": "ELT Data From S3 to Redshift",
	"tags": [],
	"description": "",
	"content": " In this section, I will create a lambda function to load the newly added objects in the S3 bucket to Redshift. Moreover, I will also create Redshift cluster, VPC, S3 endpoint and design data model for redshift relational database.  Amazon Redshift   AWS Redshift is a cloud-based data warehouse service. It\u0026rsquo;s designed to handle massive datasets, scaling up to petabytes in size.\n  Data source:\n  Data Description: Fake Customer Data Example: Customer ID: 1 Name: 'Laura Salazar' Address: '98201 Moore Cliffs Apt. 537 Jamesburgh, NV 21021' Phone: '763-957-9359x022' Age: 20 Gender: M or F Product ID: 1 Product Name: 'Canon EOS R5' Price: 1000 Amount: 100 Transaction ID: 1 Purchase Date: '2010-01-11'  Data Model:  customers table: the primary key is id products table: the primary key is product_ID transactions table:  The primary key is transaction_id The foreign key customer_ID references the id column in the customers table. The foreign key product_ID references the product_ID column in the products table.      Step-by-Step Guide: Create default Virtual Private Cloud (VPC) and S3 Endpoint Create default VPC.  Go to VPC service management console  Click Your VPCs, from the Actions menu, select Create default VPC to create default VPC.  Well done!     Create S3 Endpoint  In the left sidebar, select Endpoint, then click Create Endpoint  Set Name tag is s3-endpoint-quocbao, Service select s3 gateway, and click Create Endpoint   Create Redshift for the data warehouse  Go to Redshift service management console  Create Redshift serverless. Set namespace is quocbao-namespace. Customize admin user credentials: Admin user name is admin, Admin user password is Password123  Set workgroup is quocbao-workgroup, select default VPC and subnetscreated before.  Wait a minute for creating Redshift cluster  Create Redshift endpoint     Create Lambda function to load the newly added objects in the S3 bucket to Redshift.   Create another lambda function named: etl_s3_to_redshift\n Set Function name is etl_s3_to_redshift, Runtime type is Python 3.12, Architecture is x84_64. Select Use an existing role to set my lambda role was create before. Then click Create function.  Add lambda function code. Select Code and paste the code source below before clicking Deploy.  import json import boto3 import psycopg2 import base64 def create_table_in_redshift(cursor): users_sql=\u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS customers ( customer_id int NOT NULL, name varchar(30), address varchar(50), phone varchar(20), age int, gender varchar(10), PRIMARY KEY(customer_id) ) \u0026#34;\u0026#34;\u0026#34; cursor.execute(users_sql) products_sql=\u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS products ( product_id int NOT NULL, product_name varchar(30), price decimal, PRIMARY KEY (product_id) ) \u0026#34;\u0026#34;\u0026#34; cursor.execute(products_sql) transactions_sql=\u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS transactions ( transaction_id int NOT NULL, customer_id int NOT NULL, product_id int NOT NULL, amount int, purchase_date datetime, total_price decimal, PRIMARY KEY(transaction_id), FOREIGN KEY(customer_id) REFERENCES customers(customer_id), FOREIGN KEY(product_id) REFERENCES products(product_id) ) \u0026#34;\u0026#34;\u0026#34; cursor.execute(transactions_sql) def etl_function(row, cursor): \u0026#34;\u0026#34;\u0026#34;_summary_ Args: row (json): containing the raw transaction data \u0026#34;\u0026#34;\u0026#34; # etl users data  customer_insert=f\u0026#34;\u0026#34;\u0026#34; INSERT INTO customers VALUES(%d, \u0026#39;%s\u0026#39;, \u0026#39;%s\u0026#39;, \u0026#39;%s\u0026#39;, %d, \u0026#39;%s\u0026#39;) \u0026#34;\u0026#34;\u0026#34;%(row[\u0026#34;customer_id\u0026#34;], row[\u0026#34;name\u0026#34;], row[\u0026#34;address\u0026#34;], row[\u0026#34;phone number\u0026#34;], row[\u0026#34;age\u0026#34;], row[\u0026#34;gender\u0026#34;]) cursor.execute(customer_insert) # etl product data  product_insert=f\u0026#34;\u0026#34;\u0026#34; INSERT INTO products VALUES(%d, \u0026#39;%s\u0026#39;, %f) \u0026#34;\u0026#34;\u0026#34; % (row[\u0026#34;product_id\u0026#34;], row[\u0026#34;product_name\u0026#34;], row[\u0026#34;price\u0026#34;]) cursor.execute(product_insert) # etl transaction data  transaction_insert=f\u0026#34;\u0026#34;\u0026#34; INSERT INTO transactions VALUES(%d, %d, %d, %d, \u0026#39;%s\u0026#39;, %f) \u0026#34;\u0026#34;\u0026#34; % (row[\u0026#39;transaction_id\u0026#39;], row[\u0026#34;customer_id\u0026#34;], row[\u0026#34;product_id\u0026#34;], row[\u0026#34;amount\u0026#34;], row[\u0026#39;purchase_date\u0026#39;], row[\u0026#34;amount\u0026#34;]*row[\u0026#34;price\u0026#34;]) cursor.execute(transaction_insert) def lambda_handler(event, context): # create connection to s3  bucket_name=event[\u0026#34;Records\u0026#34;][0][\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] stream_key= event[\u0026#34;Records\u0026#34;][0][\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] # # print(event) # print(bucket_name) # print(stream_key) # bucket_name=\u0026#34;customer-datalake-quocbao\u0026#34; # stream_key=\u0026#34;data-03-06-2024/event-05-04-17.json\u0026#34; s3_cli = boto3.client(\u0026#34;s3\u0026#34;) response=s3_cli.get_object(Bucket=bucket_name, Key=stream_key) raw_data= response[\u0026#34;Body\u0026#34;].read().decode(\u0026#34;utf-8\u0026#34;) json_data = json.loads(raw_data) json_data = base64.b64decode(json_data[\u0026#34;Records\u0026#34;][0][\u0026#39;kinesis\u0026#39;][\u0026#39;data\u0026#39;]) etl_data = json.loads(json_data) print(etl_data) try: conn = psycopg2.connect( dbname=\u0026#39;dev\u0026#39;, user=\u0026#39;admin\u0026#39;, password=\u0026#39;Password123\u0026#39;, host=\u0026#39;quocbao-workgroup.211125538553.ap-south-1.redshift-serverless.amazonaws.com\u0026#39;, port=\u0026#39;5439\u0026#39; # or the correct port from Redshift console ) cur = conn.cursor() create_table_in_redshift(cur) print(etl_data) etl_function(etl_data, cur) conn.commit() conn.close() return {\u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;ETL data to redshift successful\u0026#34;} except Exception as e: return {\u0026#34;statusCode\u0026#34;: 500, \u0026#34;body\u0026#34;: str(e)}   Add psycopg2 layer for lambda function\n Create psycopg_layer Select Layer then click Create layer to create new layer.  Set Name is psycopg2_layer, and upload psycopg2_layer.zip file. Set Architecture is x86_64 and Runtime type is Python3.12. Click Create to finish this step.  Add layer for this lambda function  Select Customer layer and choose psycopg2_layer was created before. Then, click Add.     Add VPC for lambda function to connect S3 and Redshift.\n From the Configuration menu, select VPC and add our default VPC, subnet, security groups. Then, click Add to proceed to add VPC.     Add S3 trigger for lambda function\n From the etl_s3_to_redshift function of Lambda management console, click Add trigger  Choose S3 source  Set Bucket is s3/customer-data-datalake-quocbao, Event types are PUT and POST. Then, click Add to proceed to add S3 trigger for this lambda function.     "
},
{
	"uri": "/2-kinesis/generatedata/",
	"title": "Generate Customer Data",
	"tags": [],
	"description": "",
	"content": "Generate Customer Data  Let\u0026rsquo;s open the Ubuntu command line interface to configure AWS for interaction. Type aws configure  After that, you will need to enter the exact the access key and secret access key  Let\u0026rsquo;s open Visual Studio Code to create a Python file for data generation. First, you will need to install the boto3 library: pip install boto3  Create a new Python file named fakedata.py to generate data. Paste the following code into the file:  from faker import Faker import time import random import boto3 import json # List of real product names real_product_names = [ \u0026#34;Apple iPhone 13\u0026#34;, \u0026#34;Samsung Galaxy S21\u0026#34;, \u0026#34;Sony WH-1000XM4 Headphones\u0026#34;, \u0026#34;Dell XPS 13 Laptop\u0026#34;, \u0026#34;Amazon Echo Dot\u0026#34;, \u0026#34;Google Nest Hub\u0026#34;, \u0026#34;Nintendo Switch\u0026#34;, \u0026#34;PlayStation 5\u0026#34;, \u0026#34;Xbox Series X\u0026#34;, \u0026#34;Kindle Paperwhite\u0026#34;, \u0026#34;Apple Watch Series 7\u0026#34;, \u0026#34;Samsung Galaxy Tab S7\u0026#34;, \u0026#34;Sony PlayStation VR\u0026#34;, \u0026#34;GoPro HERO9\u0026#34;, \u0026#34;Bose QuietComfort 35 II\u0026#34;, \u0026#34;Canon EOS R5\u0026#34;, \u0026#34;Nikon Z6 II\u0026#34;, \u0026#34;DJI Mavic Air 2\u0026#34;, \u0026#34;Fitbit Charge 5\u0026#34;, \u0026#34;Garmin Forerunner 945\u0026#34; ] def data_generator(fake, id): \u0026#34;\u0026#34;\u0026#34; Data Description: Fake Customer Data Example: Customer ID: 1 Name: \u0026#39;Laura Salazar\u0026#39; Address: \u0026#39;98201 Moore Cliffs Apt. 537 Jamesburgh, NV 21021\u0026#39; Phone: \u0026#39;763-957-9359x022\u0026#39; Age: 20 Gender: M or F Product ID: 1 Product Name: \u0026#39;Canon EOS R5\u0026#39; Price: 1000 Amount: 100 Transaction ID: 1 Purchase Date: \u0026#39;2010-01-11\u0026#39; \u0026#34;\u0026#34;\u0026#34; name = fake.name() address = fake.address() phone = fake.phone_number() age = fake.pyint(min_value=15, max_value = 70) gender = \u0026#34;M\u0026#34; if fake.pybool() else \u0026#34;F\u0026#34; product_name= random.choice(real_product_names) amount = fake.pyint(min_value=1, max_value= 150) price = fake.pyint(min_value=100, max_value=10000) purchase_date = fake.date() return {\u0026#34;customer_id\u0026#34;: id, \u0026#34;name\u0026#34;: name, \u0026#34;address\u0026#34;: address, \u0026#34;phone number\u0026#34;: phone, \u0026#34;age\u0026#34; : age, \u0026#34;gender\u0026#34;: gender,\u0026#34;product_id\u0026#34;:id, \u0026#34;product_name\u0026#34;: product_name, \u0026#34;amount\u0026#34;: amount, \u0026#34;price\u0026#34;: price, \u0026#34;purchase_date\u0026#34;: purchase_date, \u0026#34;transaction_id\u0026#34;: id} def send_data_to_kinesis(data, kinesis_cli, stream_name, partition_key): try: kinesis_cli.put_record( StreamName=stream_name, Data=json.dumps(data), PartitionKey=partition_key ) except: print(\u0026#34;Error sending data!\u0026#34;) exit(1) if __name__==\u0026#34;__main__\u0026#34;: stream_name = \u0026#34;CustomersDataStream\u0026#34; partition_key=\u0026#34;part_1\u0026#34; fake = Faker() kinesis_client = boto3.client(\u0026#34;kinesis\u0026#34;, region_name=\u0026#34;ap-south-1\u0026#34;) id = 1 while True: data = data_generator(fake, id) send_data_to_kinesis(data, kinesis_client, stream_name, partition_key) print(f\u0026#34;Sent data: {data}\u0026#34;) id +=1 time.sleep(5) Well done! It looks like. Remember your stream key is CustomersDataStream   "
},
{
	"uri": "/2-kinesis/kinesisstreamkey/",
	"title": "Create Kinesis Stream Key",
	"tags": [],
	"description": "",
	"content": "Create Kinesis Stream Key  Return to your AWS account and type Kinesis in the search box to create a stream.  Select Kinesis Data Stream and click Create data stream  Stream key name is : CustomersDataStream, Select Provisioned data stream capacity and Provisioned shards is 1. Click Create data stream.  CustomersDataStream is created and actived.  Run the Data Generator by executing the python fakedata.py file and then view the results.  Let\u0026rsquo;s open the Amazon Kines console and check if the CustomersDataStream you created previously exists. In Data Viewer, select the shard shardid-00000000000, and set Starting posistion to Trim horizon. Click Get records to view the incoming data.   "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/5-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": " We will take the following steps to delete the resources we created in this project.  Delete Redshift Workgroup and Namespace  Go to Redshift service management console  Select Workgroup configuration. From the Actions menu, select Delete workgroup.  Enter delete, then click Delete to proceed to delete workgroup.  Select Namespace configuration. From the Actions menu, select Delete namespace.  Enter delete, then click Delete to proceed to delete namespace.     Delete S3 bucket   Go to S3 service management console\n Click on the S3 bucket we created for this project: customer-data-datalake-quocbao Click Empty.  Enter permanently delete, then click Empty to proceed to delete the object in the bucket.  Click Exit.    After deleting all objects in the bucket, click Delete\n  Enter the name of the S3 bucket, then click Delete bucket to proceed with deleting the S3 bucket.  Delete Lamdba function  Go to Lambda service management console  Click 2 lambda functions we created for this project: kinesis-event-processor-to-s3 and etl-s3-to-redshift From the Actions menu, select Delete.  Enter delete, then click Delete to proceed to delete lambda functions.     Delete Kinesis data stream  Go to Kinesis service management console  Click Kinesis data stream we created for this project: CustomersDataStream From the Actions menu, select Delete.  Enter delete, then click Delete to proceed to delete kinesis data stream     Delete VPC   Go to VPC service management console\n Click Your VPCs, select default VPC we created for this project. From the Actions menu, select Delete VPC.     In the confirm box, enter delete default vpc to confirm, click Delete to delete VPC and related resources.   "
},
{
	"uri": "/",
	"title": "Real-time Customers Data Pipeline with AWS Services",
	"tags": [],
	"description": "",
	"content": "Work with Amazon Processing - Storage - Visualization Services Overall This project aims to build a real-time data pipeline for customer data on an e-commerce platform using AWS services. This will allow the owner to gain clear insights into customer demographics(age, gender, locations, etc.), product consumption patterns, best-selling products, and more. With this information, the owner can make informed decisions to benefit the company.\n Data Ingestion: AWS Kinesis Processing data : AWS Lambda Storange data( Data Lake): AWS S3 Database(Data Warehouse): AWS Redshift Analystic (Visualization): Redshift visualization tool  Table of Contents  Introduction  Kinesis Data Ingesion Lambda Data Processing Running Data Pipeline and Visualization Redshift Data Clean up resources  "
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]